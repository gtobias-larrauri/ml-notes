\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx} % Required for inserting images
\usepackage{algpseudocodex}
% \usepackage[preprint]{neurips_2024}

\begin{document}

\title{machine learning notes}
\author{guillem.tobias }
\date{October 2024}


\maketitle
\section{TO DO LIST}
\begin{itemize}
    \item Chapter 2 exercises: 2.5,2.6,2.7,2.9(intuitive but do more mathematically). First approach done transcribe.
    \item Chapter 3 many exercises, most need quite a bit of lin alg. Check it out as lin alg is essential for exam. 
    \item Chapter 10: Boosting. \textit{Some basis expansions may be required.}
\end{itemize}

\section*{Math for ML: Chapter 7 continuous optimization.}
\textbf{Summary:} We have algorithms that will converge to a minimum - (stochastic) gradient descent - but that minimum may be a local one. 
In constrained optimization we use Lagrangian and duality. The dual problem can sometimes be easier to solve than the primal. 
In 2 cases, quadratic and linear programming problems, we will have strong duality, that is, both (primal and dual) problems give 
the exact same solution and they can be solved efficiently.  

\subsection*{Preliminaries}
\textit{Some basic rules from inverse and transpose key for matrix derivations.}
\begin{align*}
\mathbf{A A^{-1} = I = A^{-1} A \;, (AB)^{-1} = B^{-1}A^{-1} \;, (A + B)^{-1} \neq (A)^{-1} + (B)^{-1}}
\\
\mathbf{(A^T)^T = A \;, (A+B)^T = A^T + B^T \;,(AB)^T = B^T A^T}
\end{align*}

\textit{Definition of variance/covariance of a random vector}
After a few steps some stuff cancels out, we get traditional formula for variance
but this time in vector form.
\begin{align*}
\text{Var}(Z) := \mathbb{E}[(Z - \mathbb{E}(Z) )(Z - \mathbb{E}(Z))^T] 
\end{align*}
We have the equivalent for the conditional variance: 
\begin{align*}
    \text{Var}(Z) := \mathbb{E}[(Z - \mathbb{E}(Z|X) )(Z - \mathbb{E}(Z|X))^T|X]
    = \mathbb{E}[ZZ^T|X] - \mathbb{E}[\mathbb{E}(Z|X)\mathbb{E}(Z|X)^T|X]
\end{align*}


\textit{The most important fact is to check the dimension of what you are deriving! Most times it might be a hidden dot product.}

\begin{itemize}
    \item All derivatives over x.
    \item $\mathbf{a'x} = \mathbf{x'a} = \mathbf{a} $ This one is key!!
    \item $\mathbf{Ax} = \mathbf{A} $ 
    \item $ \mathbf{x'Ax} = \mathbf{(A + A')x} $
\end{itemize}

An interesting way to rewrite a constrained optimization problem is with infinite steps functions: 
$J(\mathbf{x}) = f(\mathbf{x}) + \sum_{i=1}^{m}\mathbf{1}(g_i(\mathbf{x}))$ and then we can view the lagragian 
$J(x) = \text{max}_{\mathbf{\lambda} \geq 0}\mathfrak{L}(\mathbf{x,\lambda}) $ the lagrangian is a lower bound of J(x) we can see 
this when constraints are negative and positive how max will be lambda 0 or infinity. This can be used to derive the weak 
duality results. Weak duality is not directly super useful in my very limited view as it is just a bound, although bounds can be
very useful. 

\subsection*{Linear programming and quadratic programming}

We have a linear program: $\text{max}_{\mathbf{\mathbf{x} \in \mathbb{R}^d}}\; \mathbf{c'x}\;  \text{s.t}\; \mathbf{Ax} \leq \mathbf{b} $
In Lagrangian form we get $\mathfrak{L}(\mathbf{x,\lambda})  = \mathbf{c'x} +  \mathbf{\lambda' (Ax - b)} $
To get the dual we just take derivative with respect to x plug in the FOC and add the FOC and multiplier positivity as constraints. 
The derivation has been completed the tricky step is $\lambda'A$ is a vector which makes the whole thing a dot product!

For quadratic program we can get something very similar remember the $x'Qx$ Q is symetric positive definite so tranpose is equal. 
An important example is solving a system of equations via squared error loss, OLS. 
$$||\mathbf{Ax -b}||^2 = \mathbf{Ax -b}'\mathbf{Ax -b} = \mathbf{x'A' -b'} \mathbf{Ax -b} = \mathbf{x'A'Ax} - mathbf{x'A'b} - (b'Ax) +\mathbf{b'b}$$
Notice that all terms are scalars as they should be. Then 2 and 3rd term are the same as the transpose of a scalar is the scalar. Then take derivative. 
Also while A is not symmetric, A'A is symmetric which simplifies the first derivative.

\subsection*{Convex conjugates}

\subsection*{Exercises}
7.8 from math book: min wrt w $\mathbf{w'w} \text{s.t} \mathbf{w'x}\geq 1$
Multiply inequality by -1 so everything is nicer. Set up primal lagrangian, condition is w = lambda x 
dual is a function of lambda(scalar) and data and s.t lambda being positive. 
\textit{I think this is PCA related} 

\section*{ESLII CHAP 2: Overview of supervised learning.}
This chapter contains the basic ideas and then some 

\section*{ESLII CHAP 3: Linear methods for regression.}
These two chapters are very important and linear algebra heavy (for obvious reasons).
\section*{ESLII CHAP 4: Linear methods for classification.}
Many important things here, to complement start with the All of Stats chapter. 

\begin{itemize}
    \item LDA for visualization seems very cool.
    \item MLE of a multinomial review in depth, don't want to get stuck there.
    \item LDA or logit sub-section is very important! Relationship between the 2
    \item Exercise 4.2: lda and linear regression equivalence.
    \item Exercise 4.6 with separating hyperplanes converging.
\end{itemize}


\section*{ESLII CHAP 5: Basis expansions}

\subsection*{Basics for understanding boosting.}
The core idea in this chapter is to augment/replace the
vector of inputs X with additional variables, which are transformations of
X, and then use linear models in this new space of derived input features.

Most basic splines are constants, follow the same reasoning as a tree 

Knot/boundary location is \textit{uniform over the given $x_p$} at first, I am sure there are algorithms to improve location and whatnot.

\textit{Traditional workflow:}
Take each variable and make splines out of it, can be constant or linear or cubic, whatever. Constant is clearest example.
Then these splines have parameters to be estimated, the grouped mean or whatever, if it's linear it's like a local ols.
\textbf{We run the same linear algorithm in the end, that is nice part.}

\begin{itemize}
    \item Indicator functions and OLS over disjoint regions, ols estimates will give 
    you sample mean over region.
    \item Extends to previous chapters very nicely, then coefficients interepreted as usual. 
    \item Nonparametric logistic regression and natural splines.
    \item Some RKHS 5.8 also seems interesting.
    \item Interesting exercises for SVM from infinite dimensional to low dimension.
\end{itemize}

\section*{ESLII CHAP 6: Kernel smoothing}
Have a quick look could show up somehow.



\section*{ESLII CHAP 7: Model assessment and validation}

\begin{itemize}
    \item Categorical error deviance important.
    \item k nearest neighbor bias var decomposition 
    \item idem for linear regression
    \item understand ein picture.
    \item in sample vs extra sample 
    \item AIC-BIC were mentioned in class so I should check that out. 
    \item EXERCISES SUPER IMPORTANT DO MOST OF THEM.
    \item 7.3 and 5.13 on special cross validation cases.
    \item Bootstrap for cv learn the mathematicall pr of observartion in sample blabla
\end{itemize}

We choose a loss function, it will be mean squared loss 99 percent of the times. $$L(Y,f(X))=(Y-f(X))^2$$ 

Risk is the expected loss function, expectation over what? Well, over the random stuff, so if Y is random it will be over $f(x,y)$ if Y is a fixed unkown quantity just over $f(x)$.

Then we go to test error: $Err_t= E[L(Y,\hat{f}^{-X_t,-Y_t}(X))|test: X,Y]$
This test error is random because it comes from a random sample, because the f function is generated with the training data no? ,but it can be easily computed via sample analog, for a given sample this is a known quantity. \textbf{A bit unsure.}

By LIE we get back to original definition of risk as the expected. $E_t[Err_t]$, this definition is very useful as a more practical one because we can compute everything's sample analog with ease. Compute error over samples, inner error is known no? and then average, justifies cross validation. It is analogous.
\textit{This reasoning is general, it follows for any loss function, including for classification.}

The chapter has \textbf{2 aims}: \textbf{Model selection} and \textbf{Model evaluation}, from the previous results we know that it is very similar.

In data rich scenarios, we divide into 3 parts, training, validation for the model selection, and evaluation for the test. Because model selection and evaluation are kind of similar, you don't want to tune your models hyperparameters on the test set.

Then we get back to bias variance decomposition. 
\textbf{This would be good practice to derive and to fully understand what they are doing, expectations over what blabla}

\subsection*{7.4 part of the chapter}

I think that my reasoning above was wrong as now they define it more properly.
Define the training set t.
Then $$Err_{X_0,Y_0}= E_{X_0,Y_0}[L(Y^0,\hat{f}(X^0))|t]$$
This expectation is over the distribution of the test sample. Now this is a random quantity because X0, y0 are new, this is very much CV analog.
For the unconditional error we take LIE over the distribution of the training samples, we get expected error or risk. Then this can be done in sample version very easily.
\subsection*{7.7 INTERESTING BAYES AND BIC.}


\subsection*{7.10 Cross validation}
Remember asymptotics of cross validation are tough due to correlation, read that paper and the executive summary.
Cross validation estimator of pred error (I think they should say risk). $$CV(\hat{f})=\frac{1}{N}\sum_i=L(y_i,\hat{f}^{-k(i)}(X_i))$$

They mention this is approximately unbiased estimator, but with high correlation in the sample which is what makes it complicated I am guessing.
This can be used for hyperparameter tuning, everything the same with a given alpha. 
For hyper parameter tuning we usually need to use brute force grid search.

The right way to do CV, \textit{avoid data leakage}, start with CV split and then do variable selection. 
If we have multistep modeling, like variable selection. \textbf{The CV is done in the first step and then all the rest follows as normal but applied to the K-1 folds, never the total sample.}

\subsection*{Bootstrap}
\textbf{Z}=$(z_1,...,z_N)$ where $z_i = (x_i,y_i)$ tuple, so each tuple is an iid sample.
$S(Z) $ is the statistic and we can compute the variance or confidence interval with ease.

$\hat{Var}$ = $1/(B-1)\sum_{b=1}(S(Z^b)-av(S_b))^2$

\textit{Beautiful equivalence: }Note that dVar[S(Z)] can be thought of as a
Monte-Carlo estimate of the variance of S(Z) under sampling from the
empirical distribution function  $\hat{F}$ for the data (z1, z2, . . . , zN). 

Interesting thought, so the bootstrap will converge to the variance of the estimator in the sample? And we think that sample will be representative.

Not a good estimator for expected error/risk. Can be more or less fixed.

\subsection*{Expected error or conditional error}

Review the simulation they do because it is confusing. 
\textbf{Main takeaway:} Estimating conditional is more difficult. CV /boot give unbiased estimates of the error rate.


\section*{ESLII Chap 8: Inference and bagging}
\begin{itemize}
    \item Page 285 is very interesting, do as exercise understand deeply
    \item Bagging good vs bad classifiers. Wisdom of crowds. Application in random forests.
    \item Exercises don't seem to be too good, check them out anyways, derivation in chapter can also be used as exercise.
\end{itemize}

\subsection**{Highly imbalanced classes}
https://stats.stackexchange.com/questions/235808/binary-classification-with-strongly-unbalanced-classes

https://stackoverflow.com/questions/59409967/proper-way-to-handle-highly-imbalanced-data-binary-classification
\subsection*{ESLII CHAP 8.7:  Bagging}
\textbf{TO DO READ CHAP 8 ON BOOTSTRAP} \textit{In Section 8.4 we investigated
the relationship between the bootstrap and Bayes approaches, and found
that the bootstrap mean is approximately a posterior average.}


Bootstrap aggregation can be applied to many models, it works especially well with high variance low bias models like trees.

\textbf{\textit{General procedure:}}
We have training data \textbf{Z} = $\{(x_1,y_1),...,(x_N, y_N) \}$
We fit a model to the original data and we get: $\hat{f}(x)$. OG model.

We do many boostrap samples and rerun the model at each iteration, $\hat{f}^{b_1}(x),...,\hat{f}^{b_M}(x)$

Our bagging model will be defined as $$\hat{f}_{\textrm{bag}}(x) = \frac{1}{M} \sum_{b=1}^M \hat{f}^{b_i}(x)$$

And we will of course use it on the original data. \textit{For classification we use majority vote on the predictions.}

This has very nice interpretation, if we define $\hat{E}$ as the empirical distribution.
The true bagging is defined as $\textbf{E}_{\hat{E}}(\hat{f}^*(x))$ So our estimator is a Monte Carlo approximation to it. And if we have a large enough sample the empirical distribution converges to the true distribution. \textit{Maybe we get something like exp of f(x), IDK, check statistical decision theory then jajaja. }

\textbf{EXAMPLE 8.7.1} Good, check it out, but how do they compute this Bayes error so easily check out the first weeks of theory.

With predictors we have some semi guarantees it will improve, with classifiers there are no such guarantees.

\subsection*{Focus on random forests.}
\textit{Random forests are not just bagged trees, they are bagged decorrelated trees!!}

Boosting and random forests have similar performance on a lot of problems, but are super easy to train.

Same procedure as boosting but the idea is to decrease the correlation amongst the trees to reduce the variance of the model.
Then what we do is for every bootstrap sample, at each split decision, we only consider a subset of m << p variables to split from. This decorrelates the trees quite a bit.

The expected value of the average will be the same as for one tree but we will be decreasing the variance. \textit{This argument is imprecise as the bootsrtrap samples are not iid but ok we approximate some empirical distribution blablabla} \textbf{Law of large numbers reasoning.}
Variance of i.d not independent observations will be:
$$\rho \sigma^2 + \frac{1-\rho}{B}\sigma^2$$
Deriving this reminds me of time series but I should be able to do it no?!
\textbf{TRY EXERCISES 15.1 AND 15.4}
\textit{In 15.1 the key is recognizing the number of covariance terms we will have, for that it is useful to draw a box! b(b-1)}

\subsection*{Out of bag error}
\textit{For each observation $z_i = (x_i, y_i)$, construct its random forest
predictor by averaging only those trees corresponding to boot-
strap samples in which $z_i$ did not appear.}

This can be an interesting procedure for our 

\section*{ESLII CHAP 9.2: Tree based methods.}
\textbf{Very important section!}
Trees are usually done with this recursive binary splitting, so we split the whole regressor space into 2, then we split the subsequent regions and so on and so on. If you don't stop the tree will be overfitting a lot.

Key equation that describes a tree: 
$$\hat{f}(X) = \sum_{m=1}^{N_r} c_m \textrm{\textbf{I}}(X \in R_m)$$
This seems complicated but it just means that for all observations in a given region we will predict with a constant number.

If we then use a squared loss criterion the $c_m$ tha minimizes the loss will be the conditional mean for that given subset.

It is easy start from $\sum_i^n(y_i -f(x))^2$ then plug in our f(x) and just decompose this using the indicator, so partition the n sum into different smaller sums for each subgroup.
\textit{Do the derivation as an exercise.}

Then for any general split:
We consider the following regions: $R_1(j,s) = \{ X| X_j \leq s \}$ and $R_2(j,s) = \{ X| X_j > s \}$

Our optimization problem is then to minimize the squared loss in both regions : 
$$ \textrm{min}_{j,s}[\textrm{min}_{c_1} \sum_{x_i \in R_1(j,s)}(y_i-c_1)^2 + \textrm{min}_{c_2} \sum_{x_i \in R_1(j,s)}(y_i-c_2)^2  ]$$
Where we know the inner problems are just an average within a group which is super easy to compute.

But then we can have many j and s how is this problem easy to solve then? It is discrete, infinite combinations? Or if we just take points in the sample then we have still a lot of points to compare! \textbf{ASK Milan.}

\subsection*{Pruning}
More or less intuitive but I don't think I fully understand. Weakest link prune means the part that decreases the loss by less is condensed into a single node. So you want to start this thing from the bottom. \textbf{PRUNE THE LEAVES LMAO.}

\subsection*{Classification trees}
We will normally classify to a given label via majority vote within the region.
The important definition is for node m, which represents region $R_m$ and observations $N_m$ we define: 
$$\hat{p}_{mk} = \frac{1}{N_m}\sum_{i \in R_m}(\textbf{1}(y_i = k))$$
These are the true labels, no prediction here.
We then classify according to this rule: $$k(m) := \textrm{argmax}_{k} (\hat{p}_{mk})$$
Where we loop over the k for given m region and classify as the k that has larger presence in the region.


When picking best regions we can't use the squared error loss within, so we will then be using either Gini or cross-entropy/deviance. They generalize to K categories without issue. \textit{ Missclassification error should be avoided.}  

\subsection*{Surrogate splits}
If we have missing data we can make surrogate splits, for each branch we think of the multiple cuts we could make, get the top 5 and compute them with the non missing variables.
Then if the data has missing values for the first optimal cut, we go to second and so on so on. Then thinking about the regions gets a bit complicated though no? 
What happens then? \textbf{Complete. READ also missing data 9.6 chapter.}

\begin{algorithmic}
\State $i \gets 10$
\If{$i\geq 5$} 
    \State $i \gets i-1$
\Else
    \If{$i\leq 3$}
        \State $i \gets i+2$
    \EndIf
\EndIf 
\end{algorithmic}


\textit{Intuitively this makes sense if the data is truly missing at random}
If we have categorical data we can add a "missing" category.

\section*{ESLII CHAP 10: Boosting.}
\begin{itemize}
    \item Exponential loss and AdaBoost go through full derivation 343.
    \item Then derivaation 10.16 -> It is an exercise as well.
    \item Discussion on different loss functions is also a very interesting one.
    \item Boosting trees 
    \item Gradient boosting also very important. \textbf{Not understood by me!}
    \item Read up on partial dependence plots! Also very interesting!
\end{itemize}
\subsection*{AdaBoost algorithm}
$w_i = 1/N \quad \forall \quad i$
For m in 1:M:
\begin{itemize}
    \item a. Fit $G_m(x)$ usually a tree A-B 
    \item $\text{err}_m = \frac{\sum_{n = 1}^{N} w_i \mathbb{I}(y_i \neq G_m(x))}{\sum_{n = 1}^{N} w_i}$ 
    This is a weighted error metric.
    \item $w_i = w_i \text{exp}(\alpha_m \mathbb{I}(y_i \neq G_m(x))) \quad \forall i$ Notice only the weights of the missclassified points are 
    being updated.

    % \item $\alpha_m = log((1-textrm{err}_m)/\text{err}_m)$
\end{itemize}
Final ensemble predictor is now: $$G(x) = \text{sign}[\sum_{m = 1}^{M} \alpha_m G_m (X)]$$        

Boosting is a way of fitting an additive expansion over a set of basis functions. This sounds like a mouthful but you can see it from the last estimator. 

\subsection*{Forward stagewise additive fitting}
Generally you would have a class of models $f(x) = \sum_{m = 1}^{M} \beta_m b(x;\gamma_m)$ where b depends on data and is parametrized by the gamma.
Then you would love to minimize the following:
$$\text{min}_{\left\{\beta_m, \gamma_m\right\}^M } \sum_{i = 1}^{N} \mathbb{L} (y_i, \sum_{m = 1}^{M} \beta_m b(x;\gamma_m))  $$
But this is super complicated so we can simplify it to the following algorithm, Forward stagewise additive fitting (FSAF):

Initialize $f_0(x) = 0$

For $m = 1:M$
\begin{itemize}
    \item $(\beta_m,\gamma_m) = \text{argmin}\sum_{i = 1}^{N}  \mathbb{L}(y_i, f_{m-1}(x_i) = \beta b(x_i,\gamma)) $ See the prediction will be based on a 
    weighted average of the present model and all the previous models. But the params of all previously fitted models will not be modified.
    \item Update previous models to be the ensemble with the last iteration: $f_m(x) = f_{m_1}(x) = \beta_m b(x;\gamma_m)$
\end{itemize}.00.

Once you have a final ensemble you run the classification decision.


Boosting is a special case of this.

\subsection*{Loss functions and deriving what boosting is doing 10.4-10.6}

AdaBoost is forward stagewise additive fitting when the loss is exponential. For any base "weak" classifier.

\subsection*{Boosting trees}
Forward stagewise additive fitting but the general function are now trees. These are complex models, given that we have to find the region and
then the optimal parts within the region (easy part).
An iteration of FSAF would look like: $$\hat{\Theta} = \text{argmin}_{\Theta_m} \sum_{i=1}^{N}\mathbb{L} (y_i,f_{m-1}(x_i) + T(x_i, \Theta_m))$$


\textbf{Special cases could show up in the exam!}

\subsection*{Numerical optimization via Gradient boosting}
THIS IS WHAT I NEED!

\section*{ESLII CHAP 12: SVM}
\begin{itemize}
    \item One of the most math intensive methods: need strong lagrangian + duality.
    \item In chap 4 there is some discussion on optimal separating hyperplanes for separable problems.
    \item 12.2.1 Go through these derivations.
    \item 12.3.2 SVM as a penalization model seems very interesting.
    \item 12.39 equations of HUber very interesting -> also exercise I think.
    \item Go in depth into this chapter.
    \item Good treatment and practice in math book: optmization chapter and svm
\end{itemize}

\section*{ESLII CHAP 13: nearest stuff}
\begin{itemize}
    \item Something interesting at 13.2-13.4
\end{itemize}
\subsection*{
    Lecture 3 derivation of consistency for k-NN, k=1
}
This holds a big relation with the Bayes error derivation! Some exercise like this might come up in the exam.
We start from the usual suspect $$\mathbb{E}(\mathbb{I}(Y \neq \psi(x))| X = x) $$ and 
with indicators we can usually expand this expectation. We also use the definition $\eta(x) := P(Y=1|X=x)$.
Let's go from probability theorems super basic, when will the indicator be one, 
it will be one if the outcome is one and the classifier is 0 or viceversa. Then we know that the expected
value is just the probability of those events. $P((Y=1 \cap h(x)= 0) \cup (Y=0 \cap h(x)=1) |X= x) = 
P((Y=1 \cap h(x)= 0)|X= x) + P((Y=0 \cap h(x)=1) |X=x)
$
where the events are disjoint and so the union is their sum. Now we remember the definition of a conditional 
probability: $P(A|B) = \frac{P(A = a \cap B = b)}{P(B = b)}$.
Applying this with an extra conditional will give us: 
\begin{align*}
P(Y=1 \cap h(x) = 0|X=x)
=P(Y=1|X=x,h(x)=0)\times P(h(x)=0|X=x) =\\ P(Y=1|X=x)\times P(h(x)=0|X=x) 
\end{align*}

Where the last equality follows because given x we know what the classifier will give us so that conditioning is irrelevant. 

Putting everything together we get the following expression for the risk: 
$\eta(x)\times P(h(x)=0 | X = x) + (1-\eta(x))P(h(x)=1 | X = x)$
\textit{We have shown independence of the two events by definition two events independent if their 
intersection is just the multiplication and this is what we get here.}
Then we have the question of what $P(h(x)=0 | X = x)$ is in K-NN with k=1. Well, given an X we know the nearest neighbor
and so it is $P(Y = 0|X=X_{nn})$ conditioning on X-nn implicitly conditions on x I think. \textbf{Make this reasoning more accurate 
requires some deep prob knowledge I think, I don't fully get it.In the proper exercise we have expectation outisde.}
Then we want to show that the nearest neighbor converges to x.
$$P(||X_{nn} -x|| > \epsilon) =P(\text{min}(||X_i -x|| > \epsilon)) =  P(\cap_{i=1}^n \{||X_i -x|| > \epsilon \})$$
Here we use the fact that the nearest neighbor is defined as the minimum distance in the sample and then use the typical translation 
of the minimum to all a statement with all the points in the sample. Then iid and some other stuff.

\section*{ESLII CHAP 13: Unsupervised learning.}

\section*{ESLII CHAP 15: Random forests}
\textbf{A finely tuned random forest can be a very powerful model for Task 1 of the project.} 
\begin{itemize}
    \item A lot of things to know in the random forests, specially compared to bagging and boosting. 
    \item Some important exercises here
    \item Analysis of random forests 15.4, check it out. Can understand derivation as exercise.
    \item Almost all these exercises are super interesting to learn.
    \item Exercises done 15.1, 15.3, 15.4 
\end{itemize}
Random forest gets the ideas from bagging. Average independent estimators to reduce the variance. Of course different trees will not 
be independent so we need to decorrelate them by selecting a subset (m < p) of variables on which to make the split points. \textit{I think that the majority of improvement in random forests 
comes from reducing the variance of individual estimators not by reducing the bias.}

For regression we will average the predicted value for different trees and for classification we will take a majority vote.
IID average will have a variance of $\frac{1}{B}\sigma^2$ and we see it is all great as sample grows large. On the other hand if we have an ID(identically sampled NOT independent) 
the variance of the mean will be $\rho \sigma^2 + \frac{1 - \rho}{B}\sigma^2 $ by reducing correlation we can get to what we want. 

\textbf{Out of Bag Samples:}
For each observation we construct the predictor/estimator based on trees in which a given observation did not appear.
Not sure how exactly you would do that lol. Review. 

\textbf{Variable importance:}
Impurity reductions accumulated over all splits and summed over all trees in the forest. Pse metric.
\textbf{Random forest and overfitting:}
If we have a small number of relevant predictors and a large number of noisy variables a random forest is unlikely to do well.

\subsection*{Bagging 8.7}
$$\hat{f}_{\text{bag}}(x) := B^{-1} \sum_{i=1}^{B} f^{*b}(x)$$
The estimators are trained with data from the empirical distribution $\hat{\mathcal{P}}$ which is a DISCRETE, distribution which puts 
density of 1/N, $\mathbb{E}_{\hat{\mathcal{P}}}\hat{f}_{\text{bag}}(x) $ and we do an MC simulation of it as the B go to infinity we will converge to that estimator. 

\textbf{Bagging limitations:} 1. It will work with some guarantees via MSE bias variance decomposition but that does not exist for classification. 
Then it can make it worse in terms of classification. 2. For linear estimators there should be no improvement, Exercise 15.3 must be wrong somewhere. 

Semi proof via bias variance decomposition: We have $(x_i,y_i) \sim \mathcal{P}, f_{\text{ag}}(x) = \mathbb{E}_{\mathcal{P}}(\hat{f}^*(x)) $
We can start with the risk for MSE loss: 
\begin{align*}
    \mathbb{E}_{\mathcal{P}}(Y- \hat{f}^*(x))^2 &=  \mathbb{E}_{\mathcal{P}}(Y - f_{\text{ag}}(x) + f_{\text{ag}}(x) - \hat{f}^*(x))^2 \\
         & = \mathbb{E}_{\mathcal{P}}(Y - f_{\text{ag}}(x))^2 +\mathbb{E}_{\mathcal{P}}( f_{\text{ag}}(x) - \hat{f}^*(x))^2 \\
         &\geq \mathbb{E}_{\mathcal{P}}(Y - f_{\text{ag}}(x))^2
\end{align*}
Notice on the second line the first term is the bias and the second is teh variance. 
Notice if we set the estimator to be the bagged estimator, the second term will be 0 and we get a reduction of risk.      

\subsection*{Exercise solution sketch}
\textbf{Exercise 15.1, also ps5}\\
\textbf{This is a key formula to remember.} Think of it in matrix terms for intuition, can also be 
derived by induction via the typical sum of two elements formula.
\noindent
$\text{Var}(\sum_{i=1}^{N}X_i) = \sum_{ij}^{N} \text{Cov}(x_i, x_j) = \sum_{i=1}^{N}\text{Var}(X_i) + \sum_{i,j i\neq j}^{N}\text{Cov}(x_i,x_j)$
After this result the rest is applying basic formulas. 

\textbf{Exercise 15.2: risk}\\
We have the folling model $\text{Pr}(Y=1 | X) = q + (1 -2q)* \mathbb{I}(\sum_{j=1}^{J}x_j > J/2), \quad X \sim U[0,1]^p \;, 0 \leq q \leq 0.5 \;, J \leq p. $
$$R(h^*) = \mathbb{E}_X[\eta(X)\mathbb{I}(h^*(X)=0) + (1-\eta(X))\mathbb{I}(h^*(X)=1)], \; \eta(X = \text{Pr}(Y=1|X=x) $$
You need to notice by the DGP, that if the indicator is on that means q is geq 0.5 and so one implies the other.
The go one by one, e.g, $\mathbb{E}_X[\eta (X)\mathbb{I}(h^*(X)=0)] $
this expectation is eta in the situations where the indicator is positive weighted by the probability of it being positive =  q 0.5, by the uniform distribution.
This reasoning is only correct if eta X is constant over the integration domain, and it is in this case.
The correct result is q in the end. 

\textbf{Exercise 15.4: bagging linear stuff}\\
\textit{This is a very interesting exercise!!!! Teaches to think about boostrap} 


We start with an iid sample $x_i \sim F(\mu,\sigma^2)$ and from it we make two boostrap samples ($\bar{x}_1^*, \bar{x}_2^*$) using the empirical cdf.
This CDF is discrete with mass of 1/N and it only outputs the values from the original sample. 
Let's ask ourselves about the covariance between 2 values coming from the boostrap sample. 
$\text{Cov}(x_i,x_j) = \mathbb{E}(x_i,x_j) - \mathbb{E}(x_i)\mathbb{E}(x_j)$
What is the first term? Well if the xi and xj are the same then there is covaraince and that term becomes $\mathbb{E(x_i^2)} $ which can be derived 
from the original sample. If they are not the same then the correlation is 0 because the original sample is iid. 
Then we ask what is the expectation, it is the value multiplied by the probability of that happening. And for a given value it is 1/N 
then we can get $\mathbb{E} (x_i x_j) = (\frac{\sigma^2 + \mu^2}{N} + \frac{\mu^2(N-1)}{N})$  
Rearrange and $\text{Cov}(x_i,x_j) = \frac{\sigma^2}{N}$
Then we pull out the formula for variance of a sum and get the variance of the boostrapped sample $\frac{(2N-1)\sigma^2}{N^2}$ 
and then for the covariance between the two samples we pull out the same formula again and we get $\sigma^2/N$.

To get the variance of the bagged estimator you just need to apply the sum of variances formula again. 
Notice we already know the variance of a boostrapped mean and the covariance across means, then we just follow similar reasoning as above. 
We get $$\frac{(2N -1) + (B-1)N}{B N^2}\sigma^2$$, as B goes to infinity we see the first term 
will dissapear and the second term will go to $\sigma^2/N$ which is the variance of a sample mean. IT ALL MAKES SENSE NOW.

\section{All of stats: Chap 12 Statistical decision theory}

We have a wide variety of estimators $\hat{\theta}$, how do we choose amgonst them, knowing only that the truth $\theta \in \Theta$, where the big theta is
the parameter space. 
We will be trying to measure the difference between our estimator and the truth using loss functions.
They map $\Theta * \Theta \rightarrow \mathcal{R} $
There are 5 key ones (more in ML): squared error loss, $L_p$ losses, zero-one and the kullback leibler.



\textbf{IMPORTANT:} The estimator $\hat{\theta}$ is a function of the data. And only the data. Yes the parameter will drive the data,
but for given data the estimator is the same for different truths. That is my best intuition for what comes later.

\subsection*{Risk}

\textit{Definition of risk:}
$$R(\theta,\hat{\theta}) = \mathbb{E}_{\theta} (L(\theta,\hat{\theta})) = \int L(\theta,\hat{\theta}(x)) f(\theta;x)  \,dx   $$


\textit{Intuition:}\\
This is the expected value (over data generated from truth) of the loss function.
\\\\
I think there they try to avoid Bayesian thinking here. I usually interpret as the expectation given  the true underlying data distribution\dots
I think it is a bit confusing. In the end the correct intuition is we want to compare risks of different estimators at 
all possible truths. There is only one truth but we don't know ex ante which one it is.
\textbf{Exercise:}
Do variance squared bias decompostion for the case of squared error loss.

\textbf{Example 12.2-12.3:}
In 12.2 the $\theta$ is not random, it is just unknown to the researcher. We take our expectation over that random part which is X.

In 12.3 we use the decomposition from squared error. In the first term we get something in terms of truth and sample size, so this is how we see that for unbiased
estimators we tend to go to 0 risk as the variance diminishes.
\textit{We can also get risk estimators that don't depend on true parameter!!!}

\subsection*{Bayes risk and minimax:}
It is difficult for one risk function to uniformly dominate another, so we need other ways to compare risk functions and the estimators that create them.

\textit{Definitions:}
The max risk is just picking the truth that could yield to worse perfomance, aka, maximize the risk function. $\textrm{sup}_\theta R(\theta,\hat{\theta})$ This is a 
bit pessimistic.
Bayes risk is defined as: $$ \int R(\theta,\hat{\theta}) f(\theta)\,d\theta  $$
\textit{Intuition:} It is now where in Bayes terms we can think of the possible truths, not as Bayes thinking but as a researcher that 
doesn't know which truth it is ex-ante. This can be defined with the uninformative prior. 

\textbf{Example 12.5}
This shows how we can pick one or another estimator depending on our criterion. One number summaries are imperfect though,
we should exercise caution. 

\subsection*{Bayes and minimax estimators.}
Instead of getting random estimators and getting the bayes risk, we can derive estimators such that they minimize
the bayes risk. These are called Bayes rules. We have an analog for minimax. Pick the estimator 
that minimizes the maximum.

\subsubsection{Deriving Bayes estimators:}
\textit{Preliminaries: Bayes theorem}
We are just using the definition of conditional expecation and applying marginals whatever. Continuous Bayes theorem.
$$f(\theta|x) = \frac{f(x|\theta)f(\theta)}{m(x)} = \frac{f(x|\theta)f(\theta)}{\int f(x|\theta)f(\theta) \, d\theta }$$ 

We define the posterior risk as (just the conditional risk given the data, this is definition of conditional expectation): $$ r(\hat{\theta}| x) = \int L(\theta,\hat{\theta})f(\theta|x)\,d\theta $$

Now it states that Bayes risk can be rewritten as a function of posterior risk: $$r(f,\hat{\theta}) = \int r(\hat{\theta}|x) m(x) dx$$
There is a long proof showing this equality is true.
\textbf{And now if we minimize pointwise for each x, we get the Bayes estimator}.

\textbf{IMPORTANT:} For squared error loss, the Bayes estimator is the conditional expectation.
There are a couple of other important rules.

The proof in itself is trivial, start from posterior risk and take a derivative with respect to the estimator.
FOC and you get a conditional expectation. \textit{The important part is that we have done this so that the x are fixed, if not it is impossible. Try those exercises from ML}

\textbf{Try example 12.9, not easy. Well expected value with a prior and we get posterior mean.}

\textit{Rest is not too useful, do the exercises at the end of the chapter for ML.}

\section{All of stats: Chap 22 Classification}

\section*{Homeworks:}
Have all the solutions here next week!!!!
\subsection*{HW4}
\textbf{PRIORITY: Problem 1 Part c The signs are tricky for me, signs and absolute values go case by case.}

\noindent\textbf{Problem 1}
\\
a) OLS in matrix form: $\text{min}_\beta ||y-X\beta||^2 = (y-X\beta)^T(y-X\beta)$
We will want the FOC, the first thing to do is to expand the square, that gives
$y^T y - 2y^TX\beta + \beta^TX^TX\beta$ where there are two things, first $(y-X\beta)^T = (y^T -\beta^TX^T)$ and
secondly to identify the 2 dot products are the same thing tranposed. 
The FOC is then $-2X^T y + 2X^TX\beta = 0$ Here the second term comes from the symmetry of $X^TX$ and so it is the same as it's transpose.
Rearrange and you get to the OLS estimates.


b) Now we are asked to do the same for the Ridge regression $\text{min} ||y-X\beta||^2 +\lambda||\beta||^2$
In the FOC we will get the usual OLS terms plus a penalty term $\lambda \beta^T \beta$, what we need to recognize is that the term is equivalent to $\lambda \beta^T I \beta$, 
then we can take its derivative without any issues. 
The FOC is $-2X^Ty +2X^TX\beta + 2\lambda I \beta = 0$, rearranging will get you to 
the ridge estimator. 
\subsection*{HW5}
\textbf{Ex 2, also in book}\\
\textbf{This is a key formula to remember.} Think of it in matrix terms for intuition, can also be 
derived by induction via the typical sum of two elements formula.
\noindent
$\text{Var}(\sum_{i=1}^{N}X_i) = \sum_{ij}^{N} \text{Cov}(x_i, x_j) = \sum_{i=1}^{N}\text{Var}(X_i) + \sum_{i,j i\neq j}^{N}\text{Cov}(x_i,x_j)$
After this result the rest is applying basic formulas. 

\textbf{Ex 3}\\
We have the following setup: $y_i = f(x_i) + \epsilon_i, \quad x_1,...,x_n \in [0,1]^p, \quad \epsilon_1,...,\epsilon_n \sim F(\mu,\sigma^2)$ 
$f : |f(x) - f(x')| \leq L||x- x'|| \forall x,x'$. 
\textbf{X are not random!}
We are told to prove the following bound at a given fixed point $x$: 
$$\mathbb{E}(\hat{f}(x)-f(x))^2 \leq L^2ph^2 + \frac{\sigma^2}{N(x)} $$
Where $N(x) = \sum_{i=1}^{n}\mathbb{I}(x_i \in B(x))$ and B regions are the p dimensional boxes. 

What the hell is $\mathbb{E}(\hat{f}(x)-f(x))^2$, well it is a simplified version of risk under MSE,it ignores the epsilon, also switched order, remember that -1 is cancelled out 
under a square. \textbf{Identifying the issue is half the battle!!!!} 

Let's derive the variance first:
$$\text{Var}(\hat{f}(x)) = \text{Var}(N(x)^{-1}\sum_{i=1}^{n}y_i\mathbb{I}(x_i \in B(x)))
= N(x)^{-2}\sum_{i=1}^{n}\text{Var}(y_i)\mathbb{I}(x_i \in B(x)) = \frac{\sigma^2}{N(x)}
$$
The thing to notice or required is that the X be non stocahstic which it isn't in this problem formulation. 
then we take the sigma out of the summation and realize that what is inside is the definition of N(x). 

The bias term is a bit trickier start without the square and square in the end, absolute value and square are friends when it c
comes to respecting bounds.
$$|\mathbb{E}(\hat{f}(x)) -f(x)| = |N(x)^{-1}\sum_{i=1}^{n}(\mathbb{E}(y_i) -f(x))\mathbb{I}(x_i \in B(x)) |$$
Notice that the second term does not change across i so we can use this transformation. Also expectation of yi is just f(xi) as it is non stochastic.
Applying this we get:
$$|N(x)^{-1}\sum_{i=1}^{n}(f(x_i) -f(x))\mathbb{I}(x_i \in B(x)) | \leq |N(x)^{-1}\sum_{i=1}^{n}|(f(x_i) -f(x))|\mathbb{I}(x_i \in B(x)) $$
This first bound comes from the fact that now the terms will not be canceling out (some positive and some negative, now all positive). 
We now have the L type function and we get: 
$$\leq N(x)^{-1}\sum_{i=1}^{n}L||x_i - x||\mathbb{I}(x_i \in B(x))  \leq L h \times \sqrt{p}$$
For the final step we have computed the worst case norm. The length of the p dimensional boxes is h and we compute the euclidean norm of sqrt sum of elements h squared inside.
Then do not forget to square it to get back the og expression.
\end{document}
